{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Подготовить мини-корпус (4-5 текстов или до 10 тысяч токенов) с разметкой ключевых слов.\n",
    "Желательно указать источник корпуса и описать, в каком виде там были представлены ключевые слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус - 4 статьи с новостного потрала BBC.com (https://www.bbc.com/news/world/europe). После каждой статьи есть 2 или 3 тега Related topics, это и есть ключевые слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст 1 (https://www.bbc.com/news/entertainment-arts-50432110): Robyn Crawford says relationship with Whitney Houston was 'love - open and honest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст 2 (https://www.bbc.com/news/world-europe-50447733): Yellow vest protests: More than 100 arrested as violence returns to Paris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст 3 (https://www.bbc.com/news/world-europe-50446661): Czech anti-government protesters mark anniversary of revolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст 4 (https://www.bbc.com/news/entertainment-arts-50421889): Rembrandt theft foiled at Dulwich Picture Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import pandas\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victoriaregina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    normal_forms = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    tokens = [morph.parse(token)[0] for token in tokenizer.tokenize(text)]\n",
    "    normal_forms = [token.normal_form for token in tokens if token.normal_form not in en_stopwords]\n",
    "    preproc_text = ' '.join(normal_forms)\n",
    "       \n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_txt(filename):\n",
    "    with open (filename) as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = preprocessing(open_txt('doc1.txt'))\n",
    "corpus2 = preprocessing(open_txt('doc2.txt'))\n",
    "corpus3 = preprocessing(open_txt('doc3.txt'))\n",
    "corpus4 = preprocessing(open_txt('doc4.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем ключевые (отлемматизированные) слова, выделенные авторами в отдельные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_a_1 = ['domestic abuse', 'whitney houston', 'music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_a_2 = ['france', 'paris', 'france yellow vest protests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_a_3 = ['prague', 'czech republic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_a_4 = ['art', 'rembrandt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (б) Разметить ключевые слова самостоятельно. Оценить пересечение с имеющейся разметкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже размеченные мной ключевые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_vr_1 = ['robyn crawford', 'whitney houston', 'book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_vr_2 = ['paris', 'yellow vest', 'macron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_vr_3 = ['prague', 'protest', 'babis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_vr_4 = ['rembrandt', 'burglary', 'london']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_value(kw_a, kw_vr):\n",
    "    intersection_value = len(set(kw_a).intersection(set(kw_vr)))/len(kw_a)\n",
    "    return set(kw_a).intersection(set(kw_vr)), intersection_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, которые выделил и автор, и я для текста 1: whitney houston\n",
      "Пересечение с имеющейся разметкой для текста 2: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Ключевые слова, которые выделил и автор, и я для текста 1:', list(intersection_value(kw_a_1, kw_vr_1)[0])[0])\n",
    "print('Пересечение с имеющейся разметкой для текста 2:', intersection_value(kw_a_1, kw_vr_1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, которые выделил и автор, и я для текста 2: paris\n",
      "Пересечение с имеющейся разметкой для текста 2: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Ключевые слова, которые выделил и автор, и я для текста 2:', list(intersection_value(kw_a_2, kw_vr_2)[0])[0])\n",
    "print('Пересечение с имеющейся разметкой для текста 2:', intersection_value(kw_a_2, kw_vr_2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, которые выделил и автор, и я для текста 3: prague\n",
      "Пересечение с имеющейся разметкой для текста 3: 0.5\n"
     ]
    }
   ],
   "source": [
    "print('Ключевые слова, которые выделил и автор, и я для текста 3:', list(intersection_value(kw_a_3, kw_vr_3)[0])[0])\n",
    "print('Пересечение с имеющейся разметкой для текста 3:', intersection_value(kw_a_3, kw_vr_3)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, которые выделил и автор, и я для текста 4: rembrandt\n",
      "Пересечение с имеющейся разметкой для текста 4: 0.5\n"
     ]
    }
   ],
   "source": [
    "print('Ключевые слова, которые выделил и автор, и я для текста 4:', list(intersection_value(kw_a_4, kw_vr_4)[0])[0])\n",
    "print('Пересечение с имеющейся разметкой для текста 4:', intersection_value(kw_a_4, kw_vr_4)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (в) Применить к этому корпусу 3 метода извлечения ключевых слов на выбор (RAKE, TextRank, tf*idf, OKAPI BM25)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake = RAKE.Rake(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью Rake, для текста 1:\n",
      "record deal 4.0\n",
      "tennis club 4.0\n",
      "sounded like 3.666666666666667\n",
      "gospel music 3.666666666666667\n",
      "first time 3.6\n",
      "years 2.0\n",
      "crawford 2.0\n",
      "one 2.0\n",
      "love 1.8333333333333333\n",
      "life 1.75\n",
      "whitney 1.75\n",
      "room 1.75\n",
      "houston 1.6666666666666667\n",
      "everything 1.6666666666666667\n",
      "like 1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "kw_1 = rake.run(normalize_text(open_txt('doc1.txt')), maxWords=3, minFrequency=2)[:15]\n",
    "print('Ключевые слова, выделенные с помощью Rake, для текста 1:')\n",
    "for k in kw_1:\n",
    "    print(k[0], k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью Rake, для текста 2:\n",
      "yellow vests 4.4\n",
      "mr macron 4.333333333333334\n",
      "first anniversary 4.25\n",
      "tear gas 4.0\n",
      "protests 2.0\n",
      "paris 1.6666666666666667\n",
      "city 1.6666666666666667\n",
      "year 1.5\n",
      "rioters 1.5\n",
      "police 1.4285714285714286\n",
      "saturday 1.3333333333333333\n",
      "violence 1.3333333333333333\n",
      "protesters 1.3333333333333333\n",
      "mark 1.0\n",
      "anger 1.0\n"
     ]
    }
   ],
   "source": [
    "kw_2 = rake.run(normalize_text(open_txt('doc2.txt')), maxWords=3, minFrequency=2)[:15]\n",
    "print('Ключевые слова, выделенные с помощью Rake, для текста 2:')\n",
    "for k in kw_2:\n",
    "    print(k[0], k[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью Rake, для текста 3:\n",
      "mr babis 4.583333333333334\n",
      "prime minister 4.0\n",
      "million moments 4.0\n",
      "protests 2.0\n",
      "prague 1.0\n",
      "demonstrators 1.0\n",
      "resign 1.0\n",
      "protest 1.0\n",
      "held 1.0\n",
      "speakers 1.0\n",
      "group 1.0\n",
      "understand 1.0\n"
     ]
    }
   ],
   "source": [
    "kw_3 = rake.run(normalize_text(open_txt('doc3.txt')), maxWords=3, minFrequency=2)[:15]\n",
    "print('Ключевые слова, выделенные с помощью Rake, для текста 3:')\n",
    "for k in kw_3:\n",
    "    print(k[0], k[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью Rake, для текста 4:\n",
      "quickly recovered 4.0\n",
      "gallery 2.1666666666666665\n",
      "exhibition 1.3333333333333333\n",
      "paintings 1.2\n",
      "rembrandt 1.0\n",
      "secured 1.0\n",
      "advance 1.0\n",
      "officer 1.0\n"
     ]
    }
   ],
   "source": [
    "kw_4 = rake.run(normalize_text(open_txt('doc4.txt')), maxWords=3, minFrequency=2)[:15]\n",
    "print('Ключевые слова, выделенные с помощью Rake, для текста 4:')\n",
    "for k in kw_4:\n",
    "    print(k[0], k[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_list(kw):\n",
    "    kw_list = []\n",
    "    for k in kw:\n",
    "        kw_list.append(k[0])\n",
    "    return kw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_rake_1 = keyword_list(kw_1)\n",
    "kw_rake_2 = keyword_list(kw_2)\n",
    "kw_rake_3 = keyword_list(kw_3)\n",
    "kw_rake_4 = keyword_list(kw_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords as kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью TextRank, для текста 1:\n",
      "whitney houston 0.2831375762807846\n",
      "crawford 0.2664200041163173\n",
      "likes 0.24222440493252173\n",
      "like 0.24222440493252173\n",
      "love 0.16704310641392278\n",
      "loving 0.16704310641392278\n",
      "loved 0.16704310641392278\n",
      "gospel 0.12204922338547763\n",
      "years 0.11466284289154018\n",
      "life 0.10815483245545592\n",
      "public 0.10211383160508705\n",
      "people 0.09610517945439376\n",
      "dealing 0.08897166825732525\n",
      "record deal 0.08247491735974469\n",
      "thing 0.08124297429897695\n"
     ]
    }
   ],
   "source": [
    "kw_1 = kw(normalize_text(normalize_text(open_txt('doc1.txt'))), pos_filter=[], scores=True)[:15]\n",
    "print('Ключевые слова, выделенные с помощью TextRank, для текста 1:')\n",
    "for k in kw_1:\n",
    "    print(k[0], k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью TextRank, для текста 1:\n",
      "paris 0.21875676540987998\n",
      "police 0.21322507144540784\n",
      "rioters 0.17298804026729\n",
      "including 0.1659436516455677\n",
      "protests 0.163630006805174\n",
      "protesters 0.163630006805174\n",
      "protester 0.163630006805174\n",
      "economic 0.16116683870916043\n",
      "yellow vest 0.15165510598185247\n",
      "vests 0.14964579008878567\n",
      "banks 0.1369290368497519\n",
      "bank 0.1369290368497519\n",
      "said 0.13652808079892284\n",
      "tear 0.13156836865399593\n",
      "price 0.13156836865399565\n"
     ]
    }
   ],
   "source": [
    "kw_2 = kw(normalize_text(normalize_text(open_txt('doc2.txt'))), pos_filter=[], scores=True)[:15]\n",
    "print('Ключевые слова, выделенные с помощью TextRank, для текста 1:')\n",
    "for k in kw_2:\n",
    "    print(k[0], k[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью TextRank, для текста 1:\n",
      "protested 0.30649337814485833\n",
      "protest 0.30649337814485833\n",
      "protesting 0.30649337814485833\n",
      "protests 0.30649337814485833\n",
      "czech 0.24246260428800487\n",
      "told 0.16385600931320107\n",
      "president 0.16071323626790507\n",
      "business 0.15081870930108512\n",
      "minister andrej 0.14329007492884388\n",
      "calling 0.14005306952666233\n",
      "called 0.14005306952666233\n",
      "prague 0.13871229293155993\n",
      "union 0.1387122929315599\n",
      "communist 0.1387122929315599\n",
      "party 0.1387122929315599\n"
     ]
    }
   ],
   "source": [
    "kw_3 = kw(normalize_text(normalize_text(open_txt('doc3.txt'))), pos_filter=[], scores=True)[:15]\n",
    "print('Ключевые слова, выделенные с помощью TextRank, для текста 1:')\n",
    "for k in kw_3:\n",
    "    print(k[0], k[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ключевые слова, выделенные с помощью TextRank, для текста 1:\n",
      "gallery 0.25230734346082473\n",
      "painter 0.21035326280814967\n",
      "painters 0.21035326280814967\n",
      "police 0.20043980510349454\n",
      "secured 0.19891061459319265\n",
      "security 0.19891061459319265\n",
      "said 0.19202802670564265\n",
      "paintings 0.17833041950286918\n",
      "attempt 0.17833041950286915\n",
      "attempted 0.17833041950286915\n",
      "swift response 0.15863180266369475\n",
      "takes 0.1502598905070467\n",
      "planned 0.15025989050704622\n",
      "golden 0.14630757619758863\n"
     ]
    }
   ],
   "source": [
    "kw_4 = kw(normalize_text(normalize_text(open_txt('doc4.txt'))), pos_filter=[], scores=True)[:15]\n",
    "print('Ключевые слова, выделенные с помощью TextRank, для текста 1:')\n",
    "for k in kw_4:\n",
    "    print(k[0], k[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_textrank_1 = keyword_list(kw_1)\n",
    "kw_textrank_2 = keyword_list(kw_2)\n",
    "kw_textrank_3 = keyword_list(kw_3)\n",
    "kw_textrank_4 = keyword_list(kw_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = pandas.DataFrame(index = [1,2,3,4], columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.at[1, 'text'] = corpus1\n",
    "corpora.at[2, 'text'] = corpus2\n",
    "corpora.at[3, 'text'] = corpus3\n",
    "corpora.at[4, 'text'] = corpus4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = corpora['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "df_tfidf = pandas.DataFrame(X.toarray(), columns = vec.get_feature_names(), index = [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=15):\n",
    "    \n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "        \n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_tfidf(doc):\n",
    "    tf_idf_vector = vec.transform([doc])\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords = extract_topn_from_vector(feature_names,sorted_items, 15)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_tfidf_1 = list(keywords_tfidf(docs[0]).keys())\n",
    "kw_tfidf_2 = list(keywords_tfidf(docs[1]).keys())\n",
    "kw_tfidf_3 = list(keywords_tfidf(docs[2]).keys())\n",
    "kw_tfidf_4 = list(keywords_tfidf(docs[3]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ключевые слова, выделенные с помощью tf*idf, для текста 1 :\n",
      "whitney 0.397\n",
      "houston 0.189\n",
      "crawford 0.189\n",
      "time 0.164\n",
      "like 0.157\n",
      "way 0.151\n",
      "right 0.151\n",
      "life 0.151\n",
      "know 0.151\n",
      "relationship 0.132\n",
      "going 0.132\n",
      "book 0.132\n",
      "music 0.113\n",
      "loved 0.113\n",
      "love 0.113\n",
      "\n",
      "Ключевые слова, выделенные с помощью tf*idf, для текста 2 :\n",
      "yellow 0.339\n",
      "paris 0.297\n",
      "macron 0.254\n",
      "police 0.216\n",
      "vest 0.212\n",
      "rioters 0.17\n",
      "protesters 0.17\n",
      "protests 0.167\n",
      "year 0.134\n",
      "mr 0.134\n",
      "first 0.134\n",
      "violence 0.127\n",
      "vests 0.127\n",
      "france 0.127\n",
      "city 0.127\n",
      "\n",
      "Ключевые слова, выделенные с помощью tf*idf, для текста 3 :\n",
      "babis 0.328\n",
      "million 0.234\n",
      "czech 0.234\n",
      "protest 0.187\n",
      "protests 0.185\n",
      "mr 0.185\n",
      "resign 0.14\n",
      "prime 0.14\n",
      "prague 0.14\n",
      "moments 0.14\n",
      "minister 0.14\n",
      "democracy 0.14\n",
      "business 0.14\n",
      "000 0.14\n",
      "people 0.12\n",
      "\n",
      "Ключевые слова, выделенные с помощью tf*idf, для текста 4 :\n",
      "paintings 0.399\n",
      "gallery 0.399\n",
      "security 0.2\n",
      "exhibition 0.2\n",
      "works 0.133\n",
      "swift 0.133\n",
      "staff 0.133\n",
      "secured 0.133\n",
      "response 0.133\n",
      "rembrandt 0.133\n",
      "recovered 0.133\n",
      "quickly 0.133\n",
      "planned 0.133\n",
      "officer 0.133\n",
      "intruder 0.133\n"
     ]
    }
   ],
   "source": [
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print('\\nКлючевые слова, выделенные с помощью tf*idf, для текста', i+1, ':')\n",
    "    for k in keywords_tfidf(doc):\n",
    "        print(k, keywords_tfidf(doc)[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (г) Оценить точность, полноту, F-меру выбранных методов относительно имеющейся разметки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Точность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein as l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_dist(kw, kw_target):\n",
    "    return l.normalized_distance(kw, kw_target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(kws_a, kws_m):\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for kw_a in kws_a:\n",
    "        \n",
    "        for kw_m in kws_m:\n",
    "            if lev_dist(kw_a, kw_m) < 0.6:\n",
    "                tp += 1\n",
    "                \n",
    "    fp = len(kws_m) - tp\n",
    "    fn = len(kws_a) - tp\n",
    "    \n",
    "    return tp/(tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rake = (accuracy(kw_a_1, kw_rake_1) + accuracy(kw_a_2, kw_rake_2) + \n",
    "            accuracy(kw_a_3, kw_rake_3) + accuracy(kw_a_4, kw_rake_4))/4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_textrank = (accuracy(kw_a_1, kw_textrank_1) + accuracy(kw_a_2, kw_textrank_2) + \n",
    "                accuracy(kw_a_3, kw_textrank_3) + accuracy(kw_a_4, kw_textrank_4))/4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tfidf = (accuracy(kw_a_1, kw_tfidf_1) + accuracy(kw_a_2, kw_tfidf_2) + \n",
    "             accuracy(kw_a_3, kw_tfidf_3) + accuracy(kw_a_4, kw_tfidf_4))/4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность определения ключевых слов с помощью RAKE равна:  0.128258547008547\n"
     ]
    }
   ],
   "source": [
    "print('Точность определения ключевых слов с помощью RAKE равна: ', acc_rake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность определения ключевых слов с помощью TextRank равна:  0.096875\n"
     ]
    }
   ],
   "source": [
    "print('Точность определения ключевых слов с помощью TextRank равна: ', acc_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность определения ключевых слов с помощью TF*IDF равна:  0.13020833333333334\n"
     ]
    }
   ],
   "source": [
    "print('Точность определения ключевых слов с помощью TF*IDF равна: ', acc_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(kws_a, kws_m):\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for kw_a in kws_a:\n",
    "        \n",
    "        for kw_m in kws_m:\n",
    "            if lev_dist(kw_a, kw_m) < 0.6:\n",
    "                tp += 1\n",
    "    fp = len(kws_m) - tp\n",
    "    \n",
    "    precision = tp/(tp + fp)\n",
    "    \n",
    "    return float(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(kws_a, kws_m):\n",
    "    \n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for kw_a in kws_a:\n",
    "        \n",
    "        for kw_m in kws_m:\n",
    "            if lev_dist(kw_a, kw_m) < 0.6:\n",
    "                tp += 1\n",
    "    \n",
    "    fn = len(kws_a) - tp\n",
    "    \n",
    "    recall = tp/(tp + fn)\n",
    "    \n",
    "    return float(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(kws_a, kws_m):\n",
    "    \n",
    "    r = recall(kws_a, kws_m)\n",
    "    \n",
    "    p = precision(kws_a, kws_m)\n",
    "    try: \n",
    "        f = 2 * r * p/(r + p)\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        f = 0\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure_rake = (f_measure(kw_a_1, kw_rake_1) + f_measure(kw_a_2, kw_rake_2) +\n",
    "                     f_measure(kw_a_3, kw_rake_3) + f_measure(kw_a_4, kw_rake_4))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure_textrank = (f_measure(kw_a_1, kw_textrank_1) + f_measure(kw_a_2, kw_textrank_2) +\n",
    "                     f_measure(kw_a_3, kw_textrank_3) + f_measure(kw_a_4, kw_textrank_4))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure_tfidf = (f_measure(kw_a_1, kw_tfidf_1) + f_measure(kw_a_2, kw_tfidf_2) +\n",
    "                     f_measure(kw_a_3, kw_tfidf_3) + f_measure(kw_a_4, kw_tfidf_4))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-measure определения ключевых слов с помощью RAKE равна:  0.22460317460317458\n"
     ]
    }
   ],
   "source": [
    "print('F-measure определения ключевых слов с помощью RAKE равна: ', f_measure_rake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-measure определения ключевых слов с помощью TextRank равна:  0.16830065359477125\n"
     ]
    }
   ],
   "source": [
    "print('F-measure определения ключевых слов с помощью TextRank равна: ', f_measure_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-measure определения ключевых слов с помощью TF*IDF равна:  0.22712418300653597\n"
     ]
    }
   ],
   "source": [
    "print('F-measure определения ключевых слов с помощью TF*IDF равна: ', f_measure_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (д) Описать ошибки автоматического выделения ключевых слов (что выделяется лишнее, что не выделяется); предложить свои методы решения этих проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема №1: инструменты не всегда выделяют биграммы как ключевые слова. Например, RAKE выделяет Whitney и Houston как два разных ключевых слова. При этом, я придумала, как можно считать accuracy и precision в таких случаях. Я использовала нормализованное расстояние Левинштейна с условием, что если между целевым ключевым словом и словом, которое было определено ключевым, расстояние меньше 60%, алгоритм правильно определил ключевое слово как ключевое, иными словами, true positive += 1. Однако возникает следующая проблема, связанная с вычислением recall. Для каких-то алгоритмов и текстов recall равнялся 1, при этом, на самом деле, система нашла только два (whitney, houston, music) из трех ключевых слова (domestic abuse, whitney houston, music). При этом, если бы алгоритм выделил бы следующие слова как ключевые: whitney, houston, whitney houston, music, то precision получился бы больше 1, что невозможно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможное решение: докрутить к функции, которая считает расстояние Левинштейна между словами, функцию, которая будет проверять, нашли ли уже целевому слову близкое слово из предлоденных алгоритомов. А лучшее предстказанное ключевое слово выбирать по наименьшему расстоянию между словами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема №2: MorphAnalyzer как-то криво работает для английского языка, из-за чего в список ключевых слов попадают слова типа love и loved. Конечно, разные части речи, но семантика-то одна и та же. Так же после лемматизации остаются числительные, которые не хотелось бы считать как отдельные токены словаря, которые внезапно могут стать ключевыми словами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможное решение: использовать выделение только ключевых слов-существительных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема №3 (не очень большая): по парвилам бритнакского английского тысячи выдляются запятой, отсюда в словаре два разных токена после токенизации и лемматизации: 200 и 000, потому что в статье было написано 200,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможное решение: написать регулярку, которая бы превращала такую запись числительных в привичную нам: 200000. А вообще можно в принципе расширить список стоп слов и включить в него все числительные. Хотя тогда, например, в статье про кризис 2008 года мы не сможем выделить кризис 2008 как ключевое слово, хотя нам бы, наверное, этого хотелось бы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ощущение, что все вышеперичисленные проблемы связаны с недостаточно хорошим preprocessingом: нужно сделать лучше  токенизацию и использовать лемматизацию, включить цифры в список стоп-слов, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
